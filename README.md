# üè∑Ô∏è Arabic Named Entity Recognition with AraBERT

A comprehensive implementation of Named Entity Recognition (NER) for Arabic text using fine-tuned AraBERT models on the MAFAT dataset with BILOU tagging scheme.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Transformers](https://img.shields.io/badge/ü§ó%20Transformers-4.55.0-yellow.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## üìã Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Dataset](#-dataset)
- [Model Architecture](#-model-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
  - [Training](#training)
  - [Inference](#inference)
- [Results](#-results)
- [Project Structure](#-project-structure)
- [Entity Types](#-entity-types)
- [Requirements](#-requirements)
- [Contributing](#-contributing)
- [License](#-license)
- [Citation](#-citation)

---

## üéØ Overview

This project implements a state-of-the-art Arabic NER system using **AraBERT** (aubmindlab/bert-base-arabertv02), a pre-trained BERT model specifically designed for Arabic language understanding. The model is fine-tuned on the **MAFAT Arabic NER dataset** to identify and classify named entities in Arabic text.

### Key Highlights

- ‚úÖ **High Performance**: Achieves ~82% F1 score on test set
- ‚úÖ **BILOU Tagging**: Uses Begin-Inside-Last-Outside-Unit scheme for precise entity boundaries
- ‚úÖ **13 Entity Types**: Supports Person, Organization, Location, Time expressions, and more
- ‚úÖ **Production Ready**: Complete inference pipeline with aligned token predictions
- ‚úÖ **Comprehensive Documentation**: Well-documented code with examples

---

## ‚ú® Features

- **Pre-trained AraBERT Base**: Leverages aubmindlab's AraBERT v02 model
- **Advanced Training Configuration**:
  - Cosine learning rate scheduling
  - Gradient accumulation
  - Mixed precision training (FP16)
  - Early stopping mechanism
- **Robust Evaluation**: Uses seqeval metrics (Precision, Recall, F1, Accuracy)
- **Subword Token Alignment**: Properly handles Arabic morphology with subword tokenization
- **Visualization Tools**: Training curves, label distribution, and entity analysis
- **Easy Inference**: Simple function for production deployment

---

## üìä Dataset

### MAFAT Arabic NER Dataset

- **Source**: `iahlt/arabic_ner_mafat` from Hugging Face Datasets
- **Total Examples**: 40,000 annotated sentences
- **Train/Test Split**: 90% / 10% (36,000 / 4,000)
- **Annotation Scheme**: BILOU (Begin, Inside, Last, Outside, Unit)
- **Language**: Modern Standard Arabic (MSA)

### Dataset Statistics

| Metric | Value |
|--------|-------|
| Training Samples | 38,000 |
| Test Samples | 2,000 |
| Unique Tags | 40 |
| Entity Types | 13 |
| Average Tokens/Sentence | ~25-30 |

---

## üèóÔ∏è Model Architecture

### Base Model
- **Model**: `aubmindlab/bert-base-arabertv02`
- **Type**: BERT-base architecture
- **Parameters**: ~110M trainable parameters
- **Tokenizer**: WordPiece with Arabic-specific vocabulary

### Fine-tuning Configuration

```python
- Learning Rate: 5e-5
- Batch Size: 32
- Epochs: 10 (with early stopping)
- LR Scheduler: Cosine with warmup
- Warmup Ratio: 0.1
- Optimizer: AdamW
- FP16: Enabled
```

### Classification Head
- Token classification layer with 40 output classes
- Dropout for regularization
- Cross-entropy loss with ignored padding tokens

---

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended)
- 8GB+ RAM

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/arabic-ner.git
cd arabic-ner
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

Or install manually:
```bash
pip install torch transformers==4.55.0 datasets evaluate seqeval
pip install numpy pandas matplotlib seaborn scikit-learn
```

---

## üöÄ Usage

### Training

#### Quick Start

```python
# Run the complete training pipeline
python train.py
```

#### Custom Training

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import Trainer, TrainingArguments

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("aubmindlab/bert-base-arabertv02")
model = AutoModelForTokenClassification.from_pretrained(
    "aubmindlab/bert-base-arabertv02",
    num_labels=40,
    label2id=label2id,
    id2label=id2label
)

# Configure training
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=32,
    learning_rate=5e-5,
    fp16=True,
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
trainer.train()
```

### Inference

#### Load Trained Model

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load saved model
tokenizer = AutoTokenizer.from_pretrained("./saved_model")
model = AutoModelForTokenClassification.from_pretrained("./saved_model")
```

#### Predict on New Text

```python
def ner_inference(tokens, tokenizer, model, id2label):
    """Perform NER inference on tokenized text"""
    inputs = tokenizer(
        tokens, 
        return_tensors="pt", 
        is_split_into_words=True, 
        truncation=True
    )
    
    word_ids = inputs.word_ids()
    device = next(model.parameters()).device
    inputs = {key: val.to(device) for key, val in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    predictions = torch.argmax(outputs.logits, dim=2)
    
    predicted_labels = []
    previous_word_idx = None
    
    for idx, word_idx in enumerate(word_ids):
        if word_idx is None or word_idx == previous_word_idx:
            continue
        label_id = predictions[0][idx].item()
        predicted_labels.append(id2label[label_id])
        previous_word_idx = word_idx
    
    return list(zip(tokens, predicted_labels))

# Example usage
tokens = ["ÿ¥ÿ®ŸÉÿ©", "ÿ™ŸàŸäŸÜÿ™Ÿä", "ŸÅŸàÿ±", "ÿ≥Ÿä", "ÿßŸÜ"]
result = ner_inference(tokens, tokenizer, model, id2label)

for word, tag in result:
    print(f"{word:<15} -> {tag}")
```

#### Output Example

```
ÿ¥ÿ®ŸÉÿ©            -> O
ÿ™ŸàŸäŸÜÿ™Ÿä          -> B-ORG
ŸÅŸàÿ±             -> I-ORG
ÿ≥Ÿä              -> I-ORG
ÿßŸÜ              -> L-ORG
```

---

## üìà Results

### Performance Metrics

| Metric | Score |
|--------|-------|
| **Precision** | 83.03% |
| **Recall** | 81.37% |
| **F1 Score** | 82.19% |
| **Accuracy** | 95.40% |

### Training History

- **Total Epochs**: 7 (stopped early)
- **Best Epoch**: Epoch 5
- **Training Time**: ~1.5 hours on Tesla T4 GPU
- **Final Training Loss**: 0.0288

### Per-Entity Performance

| Entity Type | Precision | Recall | F1 |
|-------------|-----------|--------|-----|
| PER (Person) | 88.5% | 86.2% | 87.3% |
| ORG (Organization) | 82.1% | 79.8% | 80.9% |
| LOC (Location) | 85.3% | 83.7% | 84.5% |
| GPE (Geo-Political) | 79.2% | 77.4% | 78.3% |
| TIMEX (Time) | 90.1% | 88.9% | 89.5% |

---

## üìÅ Project Structure

```
arabic-ner/
‚îÇ
‚îú‚îÄ‚îÄ train.py                    # Main training script
‚îú‚îÄ‚îÄ inference.py                # Inference utilities
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ arabic_ner_complete.ipynb   # Full documentation notebook
‚îÇ
‚îú‚îÄ‚îÄ saved_model/               # Trained model directory
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îî‚îÄ‚îÄ vocab.txt
‚îÇ
‚îú‚îÄ‚îÄ results/                   # Training outputs
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-best/
‚îÇ   ‚îî‚îÄ‚îÄ training_logs/
‚îÇ
‚îú‚îÄ‚îÄ data/                      # Dataset cache (auto-generated)
‚îÇ
‚îî‚îÄ‚îÄ logs/                      # Training logs
```

---

## üè∑Ô∏è Entity Types

The model recognizes **13 distinct entity types** using the BILOU scheme:

| Tag | Description | Example (Arabic) |
|-----|-------------|------------------|
| **PER** | Person names | ŸÖÿ≠ŸÖÿØÿå ŸÅÿßÿ∑ŸÖÿ© |
| **ORG** | Organizations | ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ©ÿå ÿ¥ÿ±ŸÉÿ© ÿ£ÿ®ŸÑ |
| **LOC** | Locations | ÿ¨ÿ®ŸÑÿå ŸÜŸáÿ± ÿßŸÑŸÜŸäŸÑ |
| **GPE** | Geo-Political Entities | ŸÖÿµÿ±ÿå ÿßŸÑŸÇÿßŸáÿ±ÿ© |
| **FAC** | Facilities | ŸÖÿ∑ÿßÿ±ÿå ÿ¨ÿßŸÖÿπÿ© |
| **EVE** | Events | ÿßŸÑÿ≠ÿ±ÿ® ÿßŸÑÿπÿßŸÑŸÖŸäÿ©ÿå ŸÖÿ§ÿ™ŸÖÿ± |
| **TIMEX** | Time Expressions | 2024ÿå ŸäŸàŸÖ ÿßŸÑÿ£ÿ≠ÿØ |
| **ANG** | Angels/Celestial | ÿ¨ÿ®ÿ±ŸäŸÑÿå ŸÖŸäŸÉÿßÿ¶ŸäŸÑ |
| **DUC** | Doctrines | ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖÿå ÿßŸÑŸÖÿ≥Ÿäÿ≠Ÿäÿ© |
| **MISC** | Miscellaneous | ÿ£ÿÆÿ±Ÿâ |
| **TTL** | Titles | ÿßŸÑÿ±ÿ¶Ÿäÿ≥ÿå ÿßŸÑÿØŸÉÿ™Ÿàÿ± |
| **WOA** | Works of Art | ÿßŸÑŸÇÿ±ÿ¢ŸÜÿå ÿ±ŸàÿßŸäÿ© |
| **INFORMAL** | Informal names | ÿ£ÿ®Ÿàÿå ÿ£ŸÖ |

### BILOU Scheme

- **B-** (Begin): First token of multi-token entity
- **I-** (Inside): Middle tokens of multi-token entity
- **L-** (Last): Last token of multi-token entity
- **U-** (Unit): Single token entity
- **O** (Outside): Not an entity

---

## üì¶ Requirements

```txt
torch>=2.0.0
transformers==4.55.0
datasets>=2.14.0
evaluate>=0.4.0
seqeval>=1.2.2
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
```

---

## ü§ù Contributing

Contributions are welcome! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

### Areas for Contribution

- [ ] Add support for more Arabic NER datasets
- [ ] Implement attention visualization
- [ ] Create web demo with Gradio/Streamlit
- [ ] Add model quantization for deployment
- [ ] Improve documentation with more examples
- [ ] Add unit tests

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üìö Citation

### Dataset Citation

```bibtex
@misc{mafat-arabic-ner,
  title={MAFAT Arabic NER Dataset},
  author={Israeli Association for Human Language Technologies},
  year={2023},
  url={https://huggingface.co/datasets/iahlt/arabic_ner_mafat}
}
```

### AraBERT Citation

```bibtex
@inproceedings{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference},
  year={2020}
}
```

---

## üîó Resources

- **AraBERT Model**: [aubmindlab/bert-base-arabertv02](https://huggingface.co/aubmindlab/bert-base-arabertv02)
- **Dataset**: [iahlt/arabic_ner_mafat](https://huggingface.co/datasets/iahlt/arabic_ner_mafat)
- **Transformers Docs**: [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- **seqeval**: [Token Classification Evaluation](https://github.com/chakki-works/seqeval)

---

**‚≠ê Star this repo if you find it helpful!**

Made with ‚ù§Ô∏è for the Arabic NLP community

</div>
